%!TeX root=../main.tex
% rrcf.tex
\chapter{Robust Random Cut Forest}
\label{chapter:rrcf}

In diesem Kapitel wird der Andere der auf den Testdatensatz angewendeten Verfahren, der \textbf{Robust Random Cut Forest} (von hier an RRCF) in seinen Grundzügen beschrieben. Das Kapitel orientiert sich dabei an Artikel \cite{guha2016rrcfTheory} und dem zugehörigen Supplement \cite{guha2016rrcfSup}.

\section{Notationen}
Neben den Abkürzungen \textbf{RRCF} für Robust Random Cut Forest und \textbf{RRCT} für Robust Random Cut Tree, werden in diesem Kapitel die folgenden Notationen verwendet. Diese lehnen sich an die in dem Papier \cite{guha2016rrcfTheory} benutzten an:


\begin{itemize}
\item \makebox[1.5cm][l]{$u, v, x$}  \makebox[5cm][l]{Datenpunkte}
\item \makebox[1.5cm][l]{$b$}  \makebox[5cm][l]{Eine Bitfolge mit Länge $|b|$}
\item \makebox[1.5cm][l]{$S$}  \makebox[5cm][l]{Ein Datensatz der mit $|S|$ Punkten}
\item \makebox[1.5cm][l]{$d$}  \makebox[5cm][l]{Die Anzahl der Dimensionen eines Datensatzes $S$}
\item \makebox[1.5cm][l]{$x_i$}  \makebox[5cm][l]{Der Wert von $x$ in der Dimension $i \in [1,...,d]$}
\item \makebox[1.5cm][l]{$l_i$}  \makebox[5cm][l]{Die Wertespanne eines Datensatzes $S$ in Dimension $i$}
\item \makebox[1.5cm][l]{$L_1(u, v)$}  \makebox[5cm][l]{Die Manhattan Distanz zwischen den Punkten $u$ und $v$}
\item \makebox[1.5cm][l]{$T$}  \makebox[5cm][l]{Ein RRCT}
\item \makebox[1.5cm][l]{$\mathcal{T}(S)$}  \makebox[5cm][l]{Ein RRCT, welcher über den Punkten in $S$ konstruiert wurde}
\item \makebox[1.5cm][l]{$\rho|S|$}  \makebox[5cm][l]{Eine Stichprobe von $S$ mit $|\rho|S|| = |S|*\rho$}
\item \makebox[1.5cm][l]{$f(x,S,T)$}  \makebox[5cm][l]{Die Tiefe des Punktes $x$ in dem über $S$ konstruiertem RRCT $T$}
\item \makebox[1.5cm][l]{$|M(T)|$}  \makebox[5cm][l]{Die Modellkomplexität von $T$}
\item \makebox[1.5cm][l]{$\mathbb{P}r[T]$}  \makebox[5cm][l]{Die Wahrscheinlichkeit von $T$ über seinem Datensatz konstruiert zu werden}
\item \makebox[1.5cm][l]{$\mathbb{E}_T[X]$}  \makebox[5cm][l]{Der Erwartungswert von $X$}
%\item \makebox[2cm][l]{$Disp(x,S)$}  \makebox[5cm][l]{Die Verschiebung, welche $x$ in $S$ verursacht}
%\item \makebox[2cm][l]{$CoDisp(x,S,|S|)$}  \makebox[5cm][l]{Die maximale Verschiebung, welche eine Punktegruppe inklusive $x$ in $S$ verursacht}

\end{itemize}

\section{RRCF Theory}

Der RRCF basiert auf, und ähnelt somit vielerlei dem in Kapitel \ref{chapter:iforest} vorgestellten Isolation Forest. So versucht der RRCF ebenfalls Anomalien direkt vom Datensatz zu isolieren statt ein Profil einer normalen Klasse zu definieren. Auch basiert der RRCF auf über zufällige Trennwerte konstruierten Bäumen, und mittelt sein Ergebnis aus den einzelnen Ergebnissen dieser unabhängig konstruierten Bäume.
Unterscheiden tut sich der RRCF allerdings in zweierlei Hinsicht:
\begin{enumerate}
\item Bei der Konstruktion der Bäume des RRCFs, werden die Dimensionen über die der zugrundeliegende Datensatz geteilt wird nicht uniform-zufällig, sondern nach der Größe der in ihnen vorhandenen Wertespanne der Punkte des Datensatzes gewichtet ausgewählt. So kann der Einfluss von unwichtigen Dimensionen (siehe Sektion \ref{sec:komp}) reduziert werden, und die Zugrundeliegenden Wahrscheinlichkeiten jedes Baumes über einen Datensatz bleiben konstant, unabhängig davon wie dieser Baum zustande kam.
\item Das Kriterium nach dem die Ausgabe des RRCFs berechnet wird bezieht sich nicht auf die Tiefe der Punkte, sondern auf den Effekt die eine beliebige, diesen Punkt beinhaltende  Gruppe von Punkten auf die gesamte Modellkomplexität des Baumes hat. Diese Metrik ist allgemein robuster, ins besondere können Duplikate einer Anomalie in einem Baum nicht mehr ihre Klassifizierung als solche verhindern.
\end{enumerate}

In den folgenden Sektionen werden diese Unterschiede, sowie die dem RRCF zugrunde liegenden Theoreme dargestellt.

\subsection{RRCF Aufbau}
Analog zu anderen Forest-Ansätzen aus dem Gebiet der Anomalieerkennung, besteht ein RRCF aus mehreren unabhängig voneinander konstruierten \textbf{Robust Random Cut Trees} (RRCT):
\begin{definition}[RRCT]\label{def:rrcfdef1}
Ein RRCT wird über ein Datensatz $S$ mit $d$ Dimensionen wie folgt generiert:
\begin{enumerate}
\item Wähle eine Dimension $i$ aus den $d$ Dimensionen. Dabei hat jede Dimension eine Wahrscheinlichkeit proportional zu $\dfrac{l_i}{\Sigma_d l_i}$, mit $l_i = \max_{x\in S} x_i - \min_{x \in S} x_i$ ausgewählt zu werden.
\item Wähle $X_i \sim \textnormal{Uniform}[\min_{x \in S} x_i, \max_{x \in S} x_i]$
\item Teile $S$ in $S_1 = \{ x \mid x \in S, x_i \leq X_i \}$ und $S_2 = S \setminus S_1$ und fahre rekursiv auf $S_1$ und $S_2$ fort, solange $\vert S_1\vert > 1$ beziehungsweise $\vert S_2\vert > 1$.
\end{enumerate}
\end{definition}
%Referenziere Grundlagen, hier evtl eine Tabelle als Beispiel, ?Isolation Forest als Referenz?
In Schritt 1 wird die Dimension ausgewählt über die der Datensatz bei der Konstruktion des Baumes getrennt wird. Ein wichtiger Unterschied bei der Konstruktion eines RRCT zu der Konstruktion eines Baumes in einem Isolation Forest, wie in \cite{liu2012isolation}, ist dabei, dass die zur Trennung genutzte Dimension $i$ nicht Uniform über alle Dimensionen $d$ ausgewählt wird. Stattdessen werden die Dimensionen proportional dazu wie stark die Werte der einzelnen Punkte sich in den Dimensionen unterscheiden gewichtet bevor eine von ihnen zufällig bestimmt wird.
\begin{table}%[h]
  \begin{center}
    \caption{Ein Beispiel Datensatz über 3 Dimensionen mit numerischen Werten mit $S=\{x, y, z\}$ sowie die von Definition \ref{def:rrcfdef1} in Schritt 1 berechnete Wahrscheinlichkeit $\frac{l_i}{\Sigma_d l_i}$ das $S$ in Schritt 3 über die jeweilige Dimension partitioniert wird}
    \label{tab:table1}
    \scalebox{1.15}{%
    \begin{tabular}{c|c|c|c|c} % 
      \textbf{Dimension} & \textbf{x} & \textbf{y} & \textbf{z} & $\dfrac{l_i}{\Sigma_d l_i}$\\
      %$\alpha$ & $\beta$ & $\gamma$ \\
      \hline
      1 & 5 & 10 & 6 & $\frac{5}{35}$\\
      2 & 2 & 8 & 12& $\frac{10}{35}$\\
      3 & 25 & 5 & 5& $\frac{20}{35}$\\
    \end{tabular}}
  \end{center}
\end{table}

\begin{figure}[]
\centering
\includegraphics[width=140 pt]{bilder/rrcfTree.png}
\caption{Ein möglicher, nach Definition \protect\ref{def:rrcfdef1} konstruierter RRCF über den in Tabelle \protect\ref{tab:table1} dargestellten Datensatz $S$. Die erste Partition erfolgte über die dritte Dimension mit einem nach Schritt 2 zufällig bestimmten Grenzwert von 6. Da $S_1$ darauf mehr als einen Punkt enthielt erfolgte eine weitere Partition über die erste Dimension und einen Grenzwert von 8}
\label{image:rrcfTree}
\end{figure}
In Schritt 2 wird darauf analog zum Isolation Forest Verfahren ein Trennwert $X_i$ uniform aus der Wertespanne aller Punkte $x \in S$ der in Schritt 1 ausgewählten Dimension gewählt.
In Schritt 3 wird der Datensatz $S$ dann über $X_i$ partitioniert, sodass $S1$ die  Datenpunkte enthält die in Dimension $i$ größer oder gleich groß wie $X_i$ sind und $S_2$ die verbliebenen Datenpunkte, welche in $i$ einen kleiner als $X_i$ sind. 

Beispielhaft würden in dem Datensatz von Tabelle \ref{tab:table1} bei dem ersten Durchlauf der Baumkonstruktion die Dimensionen 1, 2 und 3 mit einer jeweiligen Wahrscheinlichkeit von $\frac{1}{7}$, $\frac{2}{7}$ und $\frac{4}{7}$, als die Dimension über die $S$ partitioniert wird, ausgewählt werden. Je nach gewählter Dimension wird $X_i$ darauf aus den Wertespannen $[5,10]$, $[2,12]$ beziehungsweise $[5,25]$ uniform-zufällig gewählt. Ein möglicher RRCF, welcher sich aus dem in Tabelle \ref{tab:table1} dargestellten Datensatz ergibt ist in Abbildung \ref{image:rrcfTree} dargestellt. 

Jeder innere Knoten eines RRCTs $T = \mathcal{T}(S)$, über einen Datensatz $S$, entspricht demnach einer Partition, und enthält die entsprechende Dimension und den Grenzwert für diese Partition. Die Blätter des RRCTs entsprechen den einzelnen Punkten in $S$, welche über eine Reihe von Partitionen, entsprechend der Knoten entlang des Pfades von der Wurzel von $T$ zu dem jeweiligen Blatt, von allen anderen Punkten in $S$ isoliert wurden.



\subsection{Distanzbeibehaltung bei der RRCT Konstruktion}

Damit ein RRCF zur Anomalieerkennung eingesetzt werden kann, muss gezeigt werden, dass die RRCTs in der er die Punkte des zu untersuchenden Datensatzes auf eine Art speichert, die die Distanz zwischen den Punkten Beibehält. Ein Datenpunkt der sich im Datensatz anomal abzeichnet muss, auch in einem aus diesem Datensatz gebauten RRCT als anomal erkennbar sein. Dies ist gegeben durch folgendes Theorem (Der Beweis dieses, sowie aller folgenden Theoreme ist in dem Supplement des diesem Kapitel zugrunde liegendem Artikel zu finden):

\begin{theorem}\label{theo:distance}
Sei ein RRCT $\mathcal{T}$ über einen Datensatz $S$ mit $d$ Dimensionen konstruiert. Sei das Gewicht eines Knotens von $\mathcal{T}$ die Summe der Länge der Kanten der minimal begrenzenden Box der diesem Knoten untergeordneten Punkte $\sum_i l_i$, und sei die Baumdistanz zwischen zwei Knoten $u, v \in S$ das Gewicht des letzten gemeinsamen Vorfahrens von $u$ und $v$.
Dann ist die Baumdistanz von $u$ und $v$ mindestens $L_1(u,v)$ und in Erwartung maximal ein Vielfaches von $L_1(u,v)$ um den Faktor:
\begin{align}
\mathcal O(d \log{\frac{|S|}{L_1(u,v)}})
\end{align}
\end{theorem}

Unter Zuhilfenahme des in Kapitel \ref{chapter:iforest} formulierten Theorems {\ref{theo:ifanomalie}, welches Anomalien die folgenden beiden Eigenschaften zuweist:
\begin{enumerate}
\item Anomalien bilden eine Minderheit in ihrem Datensatz
\item Anomalien zeichnen sich durch von der Norm des Datensatzes stark abweichende Merkmalsausprägungen aus
\end{enumerate}
folgt, dass ein anomaler Punkt im Durchschnitt eine höhere $L1$ Distanz zu allen anderen Punkten, als ein normaler Punkt hat. Somit folgt nach Theorem \ref{theo:distance}, dass anomale Punkte durchschnittlich eine höhere Baumdistanz zu anderen Punkten haben, deren letzter gemeinsamer Vorfahre, demnach noch mehr Punkte im RRCT repräsentiert, und daher durchschnittlich näher an der Wurzel liegt. Da ein anomaler Punkt diese Beziehung zu allen Punkten im RRCT hat, ist diese dementsprechend durchschnittlich schneller isoliert als ein normaler Punkt. 

%\subsubsection{Beweis von Theorem \ref{theo:distance}}
%Sei für einen Datensatz $S$ $l_i$ erneut als die Wertespanne zwischen den niedrigsten und höchsten Wert von $S$ in der Dimension $i$ definiert. Sei $B(S)$ die $Minimal Bounding Box$(MBB) um alle Punkte in $S$. Sei dann $P(S) = \underset{i}{\sum} l_i$ die Summe der Seitenlängen von $B(S)$. Es ergibt sich:
%\begin{lemma} \label{lem:tren}
%Die Wahrscheinlichkeit das $u, v \in S$ durch eine Partition von $S$ nach Definition \ref{def:rrcfdef1} getrennt werden ist gegeben durch:
%\begin{align}
%\frac{1}{P(S)}\sum_i|u_i-v_i|
%\end{align}
%\end{lemma}
%$P(S)$ entspricht der Summe der Länge aller Wertespannen $l_i$ in denen in Schritt 1 und 2 von Theorem \ref{def:rrcfdef1} ein Schnittpunkt gewählt wird. $\sum_i|u_i-v_i|$ entspricht der Summe der Wertespannen, auf denen die Wahl eines Schnittpunktes $u$ und $v$ trennen würde. Das Lemma folgt.


\subsection{RRCF Instandhaltung}
In diesem Abschnitt wird gezeigt das von einem RRCT $T = \mathcal T(S)$ effizient ein Punkt $x \in S$ gelöscht oder hinzugefügt werden kann, also die jeweiligen RRCTs $\mathcal{T}(S-\{x\})$ und $\mathcal{T}(S \cup \{x\})$ effizient von $T$ abgeleitet werden können.

\subsubsection{Löschen eizelner Punkte} %Move to Implementation and reference here? Reference Binary Tree Operations
Soll ein Punkt $u$ aus dem Baum $T$ gelöscht werden, so muss lediglich der Elternknoten $k$ von $u$, welcher die Trennung mithilfe der $u$ isoliert wurde darstellt, mit gelöscht werden, und der Elternknoten von $k$ bekommt als neues Kind, dass nun verwaiste Kind von $k$.


\begin{theorem}\label{theo:wahrsch}
Sei ein RRCT $T = \mathcal{T}(S)$. Wird ein Punkt $u \in S$ wie oben skizziert gelöscht, so hat der daraus resultierende Baum die gleiche Wahrscheinlichkeit gegenüber über welche Dimensionen $T$ bei seiner Konstruktion partitioniert wird, wie ein RRCT der über $S - {u}$ konstruiert wurde. Parallel dazu hat ein RRCT der über $S \cup \{v\}$ mit $v \notin S$ konstruiert wird, die gleiche Wahrscheinlichkeit wie der RRCT der aus dem hinzufügen von $v$ zu $T$ resultiert
\end{theorem}

Dieses natürliche Verhalten gegenüber dem hinzufügen und löschen von Punkten des RRCF Verfahrens, setzt es von vielen anderen Partitionierungsverfahren ab \cite{guha2016rrcfTheory}, insbesondere auch von anderen Baum konstruierenden Anomalieerkennungsverfahren Verfahren wie das Isolation Forest Verfahren, welche die über die zu partitionierende Dimension uniform-zufällig auswählen, und somit beim erstellen eines iTrees über einen Datensatz $S-\{x\}$ einen Baum erzeugen, dessen mögliche Strukturen anderen Wahrscheinlichkeiten zugrundeliegen, als ein iTree welcher über $S$ mit  $x \in S$ erzeugt, und aus welchem darauf $x$ gelöscht wurde wurde.

Die so ermöglichten dynamischen Änderungen an den durch das RRCF Verfahren konstruierten Bäumen, ermöglicht unter anderem die effiziente Anomalieerkennung auf gestreamten Daten, da die neu eintreffenden  Punkte in die bestehenden Bäume mit eingefügt werden können, anstatt das diese von Grund auf neu gebaut werden müssten.

\begin{theorem}\label{theo:proben}
Sei $S$ eine Stichprobe eines Datensatzes. Es kann ein RRCF über $S$ gebildet werden, selbst wenn $S$ dynamisch aktualisiert wird.
\end{theorem}
Das Theorem folgt aus den bisher definierten Theorem \ref{theo:wahrsch}, welches aussagt, dass es keinen Unterschied in dem erwarteten Resultat macht, ob ein RRCT über einen Datensatz $S$ konstruiert wird, oder zuerst über einen größeren Datensatz $S'$, worauf solange Punkte aus dem resultierenden RRCT entfernt wurden, bis nur noch Punkte $x \in S$ in dem RRCT verbleiben. Jedes auf $S$ angewendete Stichprobenverfahren, welches die gewünschten Zusammenhänge beibehält, kann dementsprechend auch in einem RRCT abgebildet werden. Mit Theorem \ref{theo:wahrsch} ist der Prozess der RRCT Konstruktion unabhängig von den angewendeten Stichprobenverfahren. Soll beispielsweise eine Stichprobe von $S$ der Größe $\rho|S|$, mit $\rho < 1$ uniform-zufällig erstellt werden, so kann entweder ein RRCT über $\rho|S|$ uniform-zufällig ausgewählte Punkte von $S$ konstruiert werden, oder es können $|S|-\rho|S|$ Punkte uniform-zufällig bestimmte Punkte aus einem bestehenden RRCT über $S$ gelöscht werden. Beide Vorgehensweisen resultieren in den selben Wahrscheinlichkeiten gegenüber der Struktur, sowie den ausgewählten Dimensionen, über welche die Stichprobe partitioniert wurde für den resultierenden RRCT. Parallel dazu kann jedes weiter Stichprobenverfahren vor, oder auch abhängig von der Größe des resultierenden Baumes nach der Konstruktion des RRCT angewandt werden. Es folgt:

\begin{theorem}\label{theo:downs}
Existiert ein Verfahren welches eine Stichprobe des Datensatzes $S$ per Downsampling erstellt dann existiert für jede Downsampling Rate ein Algorithmus der einen RRCT über die Stichprobe erzeugt indem er Punkte aus dem RRCT über $S$ löscht.
\end{theorem}
Somit ist es möglich die Menge an Punkten mit der ein RRCF konstruiert wurde, nach seiner Konstruktion anzupassen. Aus Theorem \ref{theo:proben} ergibt sich weiterhin:

\begin{theorem}\label{theo:change}% In grundlagen erklären das Forest über Expectations funktioniert
Sei ein RRCT über einen Datensatz $S$ konstruiert. Sei $u \notin S$. Da wir effizient den RRCT über $S \cup \{p\}$ konstruieren können, indem wir $u$ zu $\mathcal{T}(S)$ hinzufügen, können wir effizient den erwarteten Effekt von $u$ auf die Platzierung der anderen Punkte in $S$ bestimmen, sowie die erwartete Tiefe die $u$ in $\mathcal{T}(S \cup \{u\})$ hat. 
\end{theorem}
Diese Möglichkeit, kontrafaktische Fragen gegenüber dem Einfügen von $u$ in $\mathcal{T}(S)$ effizient zu beantworten, eignet sich Intuitiv der Anomalieerkennung. So kann darüber entweder die erwartete Tiefe von $u$ bestimmt werden, um über Theorem \ref{theo:distance} den Grad der Normalität von $u$ abzuschätzen, oder es kann der Unterschied den $u$ zwischen $\mathcal{T}(S)$ und $\mathcal{T}(S \cup \{u\})$ erzeugt, bemessen werden. Eine konkrete Metrik dazu wird in der nächsten Sektion in Form des $Codisplacements$ (\textbf{CoDisps}) vorgestellt.

\section{Anomalieerkennung über RRCF}

Um zu spezifizieren wie genau ein anomaler Punkt in einem $RRCF$ erkannt wird, sei hier auf das Beispiel in Kapitel \ref{chapter:grundlagen}, der Menge bestehend aus schwarzen Kugeln und Würfeln, sowie einer grünen Kugel, zurückgegriffen. Hier lassen sich 2 Arten der Anomalieausprägung definieren:
\begin{enumerate}
\item Eine Anomalie ist einfach zu beschreiben, die grüne Kugel unterscheidet sich zwar nicht im Merkmal der Länge, aber im Merkmal der Farbe stark von den anderen Objekten der Menge. Ihre Unterscheidung von der Menge ist leicht abzugrenzen. Diese Kategorisierung ist die in Kapitel \ref{chapter:grundlagen} verwendete.
\item Die Existenz einer Anomalie in einer Menge, macht es schwieriger diese Menge zu beschreiben. So müssen die Objekte der Menge nun nicht mehr nur noch nach Form, sondern auch nach Farbe differenziert werden. Der Fokus einer Beschreibung wird von einer Mehrzahl der Objekte zu einem einzigem verschoben.
\end{enumerate}
Die beiden Definitionen von Anomalieausprägungen folgen auseinander. Das eine Anomalie über ihr hervorstechendes Merkmal einfach zu beschreiben ist, ist äquivalent  dazu, dass die Beschreibung der Merkmale einer Menge einfacher wäre, würde diese Anomalie mit ihrem besonderen Merkmal beziehungsweise ihrem besonders ausgeprägtem Merkmal nicht existieren.

Das RRCF Verfahren versucht die in Punkt 2 definierte, durch einen Punkt erzeugte Verschiebung ($Disp$) zu bestimmen. Dazu wird zuerst die Komplexität eines RRCTs definiert, um eine exakte Relation über den Effekt der im RRCT untergebrachten Punkte auf die Komplexität von diesem RRCT zu bestimmen.

\subsection{Modellkomplexität eines RRCT}

\begin{figure}[]
\centering
\includegraphics[width=140 pt]{bilder/rrcf_model_compl.png}
\caption{Ein Teilbaum $T_1$ über die Menge $S_1$, eines RRCTs $T$, dessen Wurzel in $T$ die Tiefe $r+1$ hat. Der Knoten $a$ stellt eine Partitionierung von $S_1$ in zwei Teilemengen dar. $q_0,...,q_r$ sind die Bits die die Position von $a$ in $T$ beschreiben. Quelle: \protect\cite{guha2016rrcfTheory}}
\label{image:rrcf_model_compl}
\end{figure}

Sei jedem Zweig in einem RRCT ein Bit zugeordnet. Ein linker Zweig wird durch das Bit 0 und ein rechter Zweig durch das Bit 1 gekennzeichnet. Der Platz von jedem Punkt $x$ in einem RRCT ist dann in diesem eindeutig durch die Folge an Bits entlang der Zweige von der Wurzel zu dem Punkt $x$ bestimmt. Siehe Abbildung \ref{image:rrcf_model_compl}, wo der Platz von $x$ in $T$ durch die Bitfolge $q_0,...,q_r,0,1$ definiert ist. Es bietet sich die folgende Definition \ref{def:modelkomp} der Modellkomplexität eines RRCTs an:
\begin{definition}[Tiefe eines Punktes in $x$] \label{def:tiefe}
Gegeben sei ein Satz an Punkten $S$ und sei $T = \mathcal{T}(S)$ ein RRCT über S. Sei ein Punkt $x \in S$, mit der zugehörigen Bitfolge $b$. Dann sei:
\begin{align} \label{ali:tiefe}
f(x,S,T) = |b|
\end{align}
die Tiefe von $x$ in $T$.

\end{definition}
Die Tiefe eines Knotens eines Binärbaumes entspricht der Anzahl der Zweige zwischen ihm und der Wurzel. Da sich pro Zweig ein Bit in der zugeordneten Bitfolge eines Knotens eines RRCTs ergibt, folgt die Gleichung \ref{ali:tiefe}.
\begin{definition}[Modellkomplexität] \label{def:modelkomp}
Gegeben sei ein Satz an Punkten $S$ und sei $T = \mathcal{T}(S)$ ein RRCT über S. Sei $f(x,S,T)$ mit $x \in S$ die Tiefe des Punktes $x$ in $T$. Dann ist die Modellkomplexität von T: 
\begin{align}
|M(T)| = \sum_{x\in S} f(x,S,T) \label{ali:mcomp1}
\end{align}
\end{definition}

Die definierte Modellkomplexität $|M(T)|$ entspricht somit der Summe der Länge der Bitfolgen aller Punkte in dem RRCT $T$. Anomalien in einem Datensatz sorgen somit für eine höhere Modellkomplexität, da diese nach \ref{theo:wahrsch},  durch ihre Hervorstechenden Merkmale früh im RRCT Konstruktionsprozess isoliert werden, die restlichen Punkte also in Erwartung gebündelt einen weiteren Zweig herunter schickt.
% Move Lemma über Displacementanzahl hierher
\subsection{Verschiebung der Modellkomplexität durch einen Punkt $\textbf{x}$}

Parallel zu der Modellkomplexität $|M(T)|$ ist die Modellkomplexität des RRCTs $T' = \mathcal{T}(S-\{x\})$, also des RRCTs der aus der Entfernung des Punktes $x$ aus dem RRCT $T$ nach Theorem \ref{theo:wahrsch} resultiert gegeben durch:
\begin{align}
|M(T')| = \sum_{x\in S - \{x\}} f(x,S - \{x\},T) \label{ali:mcomp2}
\end{align} 

Der Effekt den $x$ auf die Modellkomplexität von $T$ hat ist demnach:
\begin{align}\label{ali:m_dif}
|M(T)| - |M(T')|
\end{align}
Dabei ist zu beachten das der Term \ref{ali:m_dif} nur für den Effekt gilt den $x$ auf $|M(T)|$ hat, da nach Theorem \ref{theo:wahrsch} mit gegebenen $T$ und $x$, der durch das Entfernen von $x$ aus $T$ produzierte RRCT $T'$ deterministisch bestimmt ist. Umgekehrt kann aber jeder einzelne $T'$ aus beliebig vielen möglichen $T$ und $x$ hergeleitet werden, es handelt sich um eine viele-zu-einem Beziehung. Somit trifft der Term \ref{ali:m_dif} keine Aussage über den Effekt den $x$ in $T'$ auf dessen Modellkomplexität haben würde.

Ausgeweitet auf alle möglichen RRCTs $T = \mathcal{T}(S)$ und allen möglichen $T = \mathcal{T}(S - \{x\})$ ergibt sich für die erwartete Verschiebung der Modellkomplexität, die $x$ im Durchschnitt in allen $T$ verursacht:


\begin{align}
\mathbb{E}_{T} [ |M(T)| ]  -  \mathbb{E}_{T'} [|M(T')|] &=
 \sum_{T} \sum_{y\in S} \mathbb{P}r[T] f(y,S,T)  \nonumber \\
 &\qquad-\sum_{T'} \sum_{y\in S-\{x\}}\mathbb{P}r[T']f(y,S-\{x\},T')\label{ali:m_comp1} \\ 
 &=  \sum_{T} \sum_{y\in S - \{x\}} \mathbb{P}r[T] f(y,S,T) \nonumber \\
 &\qquad-\sum_{T'} \sum_{y\in S-\{x\}} \mathbb{P}r[T'] f(y,S-\{x\},T') \nonumber \\ 
 &\qquad+ \sum_{T}\mathbb{P}r[T] f(x,S,T) \label{ali:m_comp2} \\
 &= \sum_{T}\sum_{y\in S- \{x\}} \mathbb{P}r[T]\Big(f(y,S,T)-f(y,S-\{x\},T')\Big) \nonumber \\
  &\qquad+ \sum_{T}\mathbb{P}r[T] f(x,S,T) \label{ali:m_comp3}
\end{align}

Der Term \ref{ali:m_comp1} ergibt sich aus \ref{def:modelkomp} und entspricht der  durchschnittlichen Modellkomplexität aller über nach Definition \ref{def:rrcfdef1} konstruierten RRCTs $T$ und $T'$. In dem Term \ref{ali:m_comp2} ist die durchschnittliche Modellkomplexität des Punktes $x$ getrennt von der des Rest des Baumes dargestellt. Wie oben dargestellt ist nach \ref{theo:wahrsch} mit gegebenen $T$ und $x$, das Resultat $T'$ der Entfernung des Punktes $x$ aus $T$ deterministisch gegeben und es gilt somit:
\begin{align}
\sum_{T'} \sum_{y\in S-\{x\}} \mathbb{P}r[T'] f(y,S-\{x\},T') = \sum_{T} \sum_{y\in S-\{x\}} \mathbb{P}r[T'] f(y,S-\{x\},T')
\end{align}
Woraus der Term \ref{ali:m_comp3} folgt und sich folgende Definition gibt:

\begin{definition}[Verschiebung ($Displacement$) eines Punktes] \label{def:dif}
Sei ein Satz an Punkten $S$ und sei ein Punkt $x \in S$. Seien $T = \mathcal{T}(S)$ und $T' = \mathcal{T}(S- \{x\})$ RRCTs über S. Die bitweise Verschiebung die der Punkt $x$ im RRCT $T$ verursacht ist:
\begin{align}
Disp(x,S) = \sum_{T}\sum_{y\in S- \{x\}} \mathbb{P}r[T]\Big(f(y,S,T)-f(y,S-\{x\},T')\Big) 
\end{align}
\end{definition}

Zu bemerken gilt, dass die totale durch $x$ durchnitlisch verursachte Vergrößerung der Modellkomplexität gegeben ist durch:
\begin{align}
\mathbb{E}_{T} [ |M(T)| ]  -  \mathbb{E}_{T'} [|M(T')|] = Disp(x,S) + \sum_{T}\mathbb{P}r[T] f(x,S,T)
\end{align} 
, also der Summe der Bits die zu der Bit-Repräsentation der Punkte $y \in S-\{x\}$ durch $x$ hinzukommen, plus der Bits die $x$ selbst darstellen.
Der Fokus der Anomalieerkennung durch RRCFs liegt demnach auf der Erkennung eines Steigens der Komplexität des Datensatzes den ein Punkt des Datensatzes hervorruft, anstatt auf dem Hervorstechen des Punktes an sich.
Die Benutzung des Wortes Verschiebung, lässt sich über folgendes Lemma herleiten:
\begin{lemma} \label{lem:verschiebung} % Move to section before this?
Die in durch einen Punkt $x \in S$ verursachte Verschiebung in einem RRCT $T = \mathcal{T}(S)$ entspricht der Menge an Punkten, die Geschwister von $x$ sind 
\end{lemma}

\paragraph{Beweis Lemma \ref{lem:verschiebung}}
Orientiert an Abbildung \ref{image:rrcf_model_compl}, ist die Bitrepräsentation jedes Punktes in $c$, also jedes Punktes welcher in dem Baum $T$ ein Geschwister von $x$ ist, gegeben durch:
\begin{align}
q_0,...,q_r,0,0,...
\end{align}

\begin{figure}[]
\centering
\includegraphics[width=140 pt]{bilder/rrcf_model_compl2.png}
\caption{Ein Teilbaum $T_2$ über die Menge $S_2$, eines RRCTs $T$, dessen Wurzel in $T$ die Tiefe $r+1$ hat. Der Knoten $a$ stellt eine Partitionierung von $S_2$ in zwei Teilemengen da. $q_0,...,q_r$ sind die Bits die die Position von $a$ in $T$ beschreiben. Quelle: \protect\cite{guha2016rrcfTheory}}
\label{image:rrcf_model_compl2}
\end{figure}

Repräsentiert in Abbildung \ref{image:rrcf_model_compl2}, welche den Teilbaum darstellt der sich aus dem Entfernen von $x$ aus dem in \ref{image:rrcf_model_compl} dargestellten RRCT ergibt, fällt durch das Entfernen nach\ref{theo:wahrsch}, von $x$ aus $T$ ein Knoten auf dem Pfad der Wurzel von $T$ zu den Punkten in dem Bereich $c$ weg, womit sich für diese eine neue Bitepräsentation gibt:
\begin{align}
q_0,...,q_r,0,...
\end{align}
Da der Pfad von der Wurzel von $T$, zu allen Knoten außerhalb des Bereiches $c$ durch das Löschen von $x$ unverändert bleibt, ergibt sich beziehend auf die Definition \ref{def:tiefe}, für den Effekt von $x$ auf die Länge der Bitrepräsentation jedes anderen Punktes in $T$:  

\begin{equation}
f(y,S,T) - f(y,S-\{x\},T') = \begin{cases}
      1, &  y \in c \\
      0, & \text{otherwise}
    \end{cases}
\end{equation}

Es folgt für die Verschiebung von $x$ in einem gegeben Baum $T$:
\begin{align}
Disp_T(x,S) = |c|
\end{align}

\subsection{Codisp}


Definition \ref{def:dif} bietet eine Möglichkeit der Anomaliedefinition. Diese ist allerdings stark anfällig gegenüber Duplikaten, wie in Sektion \ref{sec:komp} definiert. Enthält die oben definierte Menge an Objekten 2 grüne Kugeln, so würde das Entfernen einer Kugel die Komplexität der Beschreibung der Menge nicht wesentlich vereinfachen. Ein genaueres Beispiel ergibt sich wie folgt:

 Bei dem durch Abbildung \ref{image:dup-rrcf} dargestellten Datensatz $S$, würde ein auf diesem konstruierten RRCT die Punkte $o_1$ und $o_2$, basierend auf Theorem \ref{theo:distance}, aufgrund ihrer hohen Distanz $L_1$ zu allen anderen Punkten des Datensatzes wahrscheinlich schnell isolieren. $Disp(o_1,S)$ sowie $Disp(o_2,S)$ wären, aufgrund ihrer somit folgenden hohen Anzahl an Punkten in Geschwisterknoten ebenfalls hoch im Vergleich zu den Punkten in $N_1$ und $N_2$. Die Punkte in $O_3$ würden in Erwartung, aufgrund ihrer hohen Distanz $L1$, ebenfalls schnell von allen Punkten nicht in $O_3$ getrennt werden. Aufgrund ihrer geringen Distanz $L1$ untereinander würde die dafür verantwortliche Partitionierung in Erwartung alle Punkte in $O_3$, nach Schritt 3 der Definition \ref{def:rrcfdef1} in eine Teilmenge partitionieren. In Erwartung ergibt sich somit ein RRCT wie in Abbildung \ref{image:dup-rrcf}. %RRCT mit o1 und o3
 Da jedes Blatt, welches einen Punkt von $O_3$ enthält, eine geringe Anzahl an Blättern hat die von seinem Geschwisterknoten abstammen, ist somit $Disp(o_3,S)$ für alle $o_3 \in O_3$ gering. Die Punkte $O_3$ können über Definition \ref{def:dif} nicht als Anomalie erkannt werden.
 
\begin{figure}[]
\centering
\includegraphics[width=200 pt]{bilder/anomaly_chandola.png}
\caption{Ein Beispieldatensatz mit zwei Anomalien $o_1$ und $o_2$, sowie eine Punktegruppe $O_3$ von 7 Anomalien. Die Gruppen $N_1$ und $N_2$ stellen die Inliner des Datensatzes da. Quelle: \protect\cite{chandola2009anomaly}}
\label{image:dup-rrcf}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=100 pt]{bilder/rrcfDupli.png}
\caption{Ein Teilbaum, welcher }
\label{image:dup-rrcf-tree}
\end{figure}

\subsubsection{Robustheit gegenüber Duplikaten}
Um über die Modellkomplexität einen anomalen Punkt $x \in S$ als solchen zu erkennen selbst wenn $S$ Duplikate oder Beinah-Duplikate von $x$ enthält, muss demnach das Vergleichsmodell betrachtet werden, bei dem ein Satz an Punkten $C$, mit $x \in C$ entfernt wurden. Analog zu Term \ref{ali:m_comp3} ergibt sich für den erwarteten durchschnittlichen Unterschied in der Modellkomplexität aller RRCTs $T = \mathcal{T}(S)$ und $T'' = \mathcal{T}(S - C)$:
\begin{align}
\mathbb{E}_{T} [ |M(T)| ]  -  \mathbb{E}_{T'} [|M(T'')|] &= Disp(C,S) + \sum_{T}\sum_{y \in C}\mathbb{P}r[T] f(y,S,T)
\end{align}

, wobei $Disp(C,S)$ der erwarteten Bit-Verschiebung in allen Punkten $C-S$, die die Punkte $C$ im Durchschnitt über alle $T$ verursachen entspricht:
\begin{align} \label{ali:difc}
Disp(C,S) &= \sum_{T}\sum_{y \in S - C}\mathbb{P}r[T] \Big(f(y,S,T) - f(y,S - C,T'')\Big)
\end{align}


Die Bit-Verschiebung von $x$ entspricht damit, basierend auf Term \ref{ali:difc} und der Annahme, dass alle Punkte in $C$ die gleiche Bit-Verschiebung zugeschrieben werden sollte, da es sich bei diesen in Erwartung um Duplikate oder Beinah-Duplikate von $x$ handelt $Disp(C,S)/|C|$.
Dementsprechend wäre eine Methodik, $C$ zu wählen die Ermittlung des folgenden Maximums: 
\begin{align} \label{ali:difcmax}
\underset{x \in C \subseteq S}{max} Disp(C,S)/|C|
\end{align}
Dieser Methodik folgen allerdings zwei Probleme:
\begin{enumerate}
\item Die mögliche Anzahl an Sätzen von Punkten $x \in C \subseteq S$ wächst exponentiell zu $S$, weshalb Anomalieerkennung über Methodik \ref{ali:difcmax} ineffizient wäre.
\item Wird $S$ gestreamed, und der RRCF live über den Stream konstruiert, sind zum Zeitpunkt der Bewertung von $x$ noch nicht alle Punkte von $S$, also nicht alle möglichen Punkte von $C$, sondern nur ein Teilsatz $S' \subset S$ bekannt. Somit ist Methodik \ref{ali:difcmax} nicht für Streaming-Daten geeignet.
\end{enumerate}

Zur Lösung dieser Probleme, darf $C$ für unterschiedliche Teilsätze $S'$ verschieden gewählt werden. Es ergibt sich die folgende Definition des $Collusive Displacements (Codisp)$, oder der Bit-Verschiebung mithilfe einer Gruppe von Punkten eines Punktes:
\begin{definition}[CoDisp]
Sei ein Datensatz $S$ gegeben. Die erwartete durchschnittliche Bit-Verschiebung eines Punktes $x$ in allen möglichen RRCTs $T = \mathcal{T}(S')$ über ein Sample $S' \subset S$ und die darüber gegebenen $T'' = \mathcal{T}(S - C)$, ist gegeben durch:
\begin{align}
CoDisp(x,S,|S'|) = \underset{S' \subseteq S,T}{\mathbb{E}}\bigg[\underset{x \in C \subseteq S}{max} \frac{1}{|C|} \sum_{y \in S-C} f(y,S',T)-f(y,S'-C,T'' \bigg]
\end{align}
\end{definition}

Dabei kann $T''$ wieder aufgrund von Theorem \ref{theo:wahrsch} deterministisch von allen Kombinationen von $T$ und $C$ abgeleitet werden. 
Mit der nun durch $Codisp$ gegebenen, gegen Duplikate robusten Möglichkeit der Ermittlung des Effektes den ein Punkt innerhalb eines RRCTs auf die Modellkomplexität seines RRCTs hat, ergibt sich die zentrale Definition der Anomalieerkennung eines RRCFs:
\begin{definition}
Die Anomalien eines Datensatzes haben in einem über den Datensatz, oder über einem Sample über den Datensatz konstruierten RRCT in Erwartung einen hohen CoDisp() 
\end{definition}

Weiterhin gilt:

\begin{lemma}\label{lem:codispeff}
Die CoDisp(x,Z,|S|) kann effizient bestimmt werden
\end{lemma}

\paragraph{Beweis von Lemma \ref{lem:codispeff}}

Analog zu dem Beweis von Lemma \ref{lem:verschiebung} ist der U

\section{Implementation}
